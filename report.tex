\documentclass[a4paper,11pt]{article}

\usepackage{url}

\title{CS422 Project: Searching for Markovian Malware C\&C on Twitter}
\author{Maxime Augier and Gowthami Ramasamy}

\begin{document}

\maketitle



\section{Project steps}

\paragraph{Data collection}

We used the archive.org twitter stream and uploaded it to the lab cluster hdfs. The original file is a tar archive containing bzipped fragments of text files. The chunks were small, in the order of 1.5MB, so we repacked them into bigger chunk. After repacking gzip compression was used. Although the gzip format is inferior as it does not allow to seek within the file, bzip2 compression is significantly slower. One could also have concatenated the bzip fragments by fiddling with the headers but we preferred the safe approach.

For this operation we used the python code under the ``extraction'' directory. It offers a generator-based API to process the tweets. There is also a script to upload data to a MongoDB instance.

\paragraph{Early assessment}

Using Java MapReduce jobs proved extremely troublesome. The Hadoop version available was rather obsolete (1.x) and much of the documentation available on the net referenced non-existing mechanisms. Not having administrator access restricted our options; in particular, inclusion of third-party libraries for json parsing was much more difficult than needed. We tried several approaches (fat jars, trickery with the configuration object) to no avail. In the end, it turned out to be much simpler to write jobs using the streaming API. We wrote jobs in Python, as the version installed on the machine already had all the desired libraries.

Recognizing base64 turned out to be harder than expected. Regex matching yielded a significant number of false positives.

We did, however, identify one odd pattern: sequences of nucleotids (ATGC letters) in long strings. We found a couple of users tweeting these sequences as ``Human DNA pieces''. While we initially thought this may be a covert communication channel, the sequences we tested turned out to be genuine human DNA, present in the publicly-available BLAST nucleotid database.

We attempted to match base64-encoded data with the heuristic of attempting a base64 decode, then keeping only the messages for which there is at least one ascii letter. Unfortunately this also produced too many false positives.


\paragraph{Entropy estimation}

On the contents of every tweet, we computed the simple entropy of the message as a sequence of characters. Here is the CDF of entropy per tweet:

(insert graph here)

\paragraph{Model building}

MapReduce is especially well fitted to compute the probability distributions over a Markov model. We will compute our models both on letters (n-grams) and tokenized words (for the english corpus only). We also need to test several tokenization models.As a part of n-gram model and tokenized words, a MapReduce program has been developed, the logic of the model follows
\begin{flushleft}

Mapper [n-gram]:

Input: The twitter stream, without any pre-processing

Functionality:  1. The twitter stream is parsed and the language field - 'Lang' 					and Tweet text 'Text' gets extracted.

				2. The language fields is used according to the model that we are 					build [full one or English only]

				3. The tweet text further parsed into n-grams, without elimination 				any bytes.

Output: Each n-gram will be passed as a key to the reducer. The value is one. 
		{n-gram, one}

Mapper [tokenized words]:

Input: The twitter stream, without any pre-processing

Functionality:	1. The twitter stream is parsed and the language field - 'Lang' 					and tweet text 'Text' gets extracted.

				2. The language fields is used according to the model that we are 					build [full one or English only]
				
				3. The tweet text further parsed into words, tweet texts always 					contains special characters,plural forms which should be 							eliminated. So regex pattern is used to extract only alpha-numeric 				characters 

Output: Each word will be passed as a key to the reducer. The value is one. 
		{word, one}
				
Reducer [same for n-gram and tokenized words]:

Input: {Key, Value}

Functionality: Count the Key

Output: Key Count 
\end{flushleft}

Another option we want to consider (but less likely to give results) is encoding messages in deliberate grammar or orthography mistakes, trough permutation of simple characters. Decoding would be performed by a standard spell checker like aspell.

\paragraph{Channel building}

We propose to use an inverted form of Huffman coding to encode arbitrary bits into human-looking chains. The algorith wil work as follows:

First, prune the model to keep it down to a reasonable size, by excluding uncommon words.

Then, for every state in the chain, build a Huffman tree over the distribution for the next symbol.

To encode, begin with an empty starting state in the Markov chain. Perform a Huffman decoding operation on the tree of the current state, which will consume an arbitrary number of bits and produce a symbol. Output the symbol, compute the new state, and repeat until no bits are left, or the chain picks a terminating symbol, or the maximum output size is exceeded.

To decode, begin with the same empty state; for each token encoutered, perform Huffmann compression, repeat until the message is complete.

\paragraph{Entropy estimation}

Once the encoding channel is working, we shall make it run with random input bits, and measure on average how many bits fit in each tweet, for all our models (n-grams, english n-grams, english words). Combined with compression, we will estimate how many messages would be required for typical C\&C operations, checking in how many bits we can fit (for instance) a DDoS attack target or a typical shady url.

\paragraph{Detection}

The difficult part will be extracting suspicious messages conforming too well to our model. 

One first technique will be to use different Markov models of different orders, and see if for given accounts, their distribution of tweets follows a low-order model anormally better than a high-order one.

We will also use the n-gram table to compute likelihood for a set of hashtags, and try to extract randomly-generated hashtags from it. We will apply the same n-gram analysis to messages contents, and compare with the first naive regex-based classifier.

\section{Milestones}

\begin{description}
	\item[01.04.2014] Have a sample set of data ready in case the global collection effort does not work (MA), write a MapReduce implementation to build n-gram datasets (GR). Write MapReduce simple matchers (MA).
	\item[08.04.2014] Results from early assessment and detection
	\item[22.04.2014] Proof of concept for stealthy channel, MapReduce chain trimmer (GR), compressor/decompressor sample code (MA)
	\item[06.05.2014] Integrate final data sources and start looking for actual bots
	\item[13.05.2014] Final report 
\end{description}

\appendix

\section{Appendix A: results}

\begin{table}
\begin{tabular}{|l|r|}
\hline
en & 46324982 \\
ja & 22399870 \\
es & 15817824 \\
id & 10970703 \\
ar & 9368831 \\
pt & 7189249 \\
und & 3964165 \\
tr & 3897656 \\
fr & 2947715 \\
ru & 2566648 \\
tl & 1917014 \\
ko & 1730077 \\
th & 1664855 \\
it & 1029948 \\
nl & 1019123 \\
de & 725735 \\
vi & 572220 \\
et & 498251 \\
pl & 453255 \\
sl & 429062 \\
ht & 350093 \\
sk & 263615 \\
sv & 248050 \\
lv & 209531 \\
hu & 119744 \\
bg & 115099 \\
el & 111589 \\
fi & 110380 \\
da & 110267 \\
lt & 82100 \\
zh & 77609 \\
no & 71175 \\
fa & 69998 \\
uk & 57837 \\
he & 54388 \\
ur & 37947 \\
is & 35487 \\
hi & 19889 \\
ta & 8417 \\
bn & 6140 \\
ne & 5859 \\
hy & 2828 \\
ka & 2149 \\
ml & 1159 \\
te & 1071 \\
pa & 892 \\
bo & 746 \\
si & 638 \\
my & 634 \\
iu & 505 \\
gu & 492 \\
kn & 444 \\
km & 433 \\
lo & 315 \\
am & 196 \\
chr & 184 \\
dv & 146 \\
or & 47 \\
\hline
\end{tabular}
\label{twperlang}
\caption{Tweets per language}
\end{table}

\end{document}
